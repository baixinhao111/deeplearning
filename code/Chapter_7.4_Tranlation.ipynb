{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关模块\n",
    "import time\n",
    "import string\n",
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as DataSet\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -i https://pypi.tuna.tsinghua.edu.cn/simple jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.688 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'I', 'were', 'you,', \"I'd\", 'want', 'to', 'know', 'what', 'Tom', 'is', 'doing', 'right', 'now']\n",
      "['B', '如果', '我', '是', '你', '，', '我', '不会', '想', '去', '知道', 'Tom', '现在', '正在', '做', '什么', '。']\n"
     ]
    }
   ],
   "source": [
    "### 定义读取语料方法\n",
    "def read_corpus(path):\n",
    "    English = []\n",
    "    Chinese = []\n",
    "\n",
    "    f = open(path, 'r', encoding = 'utf-8')\n",
    "    for line in f.readlines():\n",
    "        eng, chs = line.strip().split('\\t')\n",
    "\n",
    "        eng = eng[:-1]\n",
    "        eng = eng.split(' ')\n",
    "        English.append(eng)\n",
    "\n",
    "        chs = jieba.lcut(chs)\n",
    "        chs = ['B'] + chs\n",
    "        Chinese.append(chs)\n",
    "    return English, Chinese\n",
    "\n",
    "English, Chinese = read_corpus('../dataset/cmn.txt')\n",
    "print(English[20000])\n",
    "print(Chinese[20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文字典大小 13683\n",
      "英文字典大小 7814\n"
     ]
    }
   ],
   "source": [
    "### 定义中英文字典编码方法\n",
    "def lang_encode(language):\n",
    "    lang2idx = {}\n",
    "    i = 1\n",
    "    for chs in language:\n",
    "        for c in chs:\n",
    "            if lang2idx.get(c) == None:\n",
    "                lang2idx[c] = i\n",
    "                i += 1\n",
    "    return lang2idx\n",
    "\n",
    "chs2idx = lang_encode(Chinese)\n",
    "eng2idx = lang_encode(English)\n",
    "chs_vocab_size = len(chs2idx.keys()) + 1\n",
    "eng_vocab_size = len(eng2idx.keys()) + 1\n",
    "\n",
    "print('中文字典大小', chs_vocab_size)\n",
    "print('英文字典大小', eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始中文: \n",
      "['B', '如果', '我', '是', '你', '，', '我', '不会', '想', '去', '知道', 'Tom', '现在', '正在', '做', '什么', '。']\n",
      "\n",
      " 中文编码后的结果: \n",
      "[1, 917, 12, 35, 5, 79, 12, 16, 257, 36, 73, 202, 429, 496, 129, 299, 3]\n",
      "原始英文: \n",
      "['If', 'I', 'were', 'you,', \"I'd\", 'want', 'to', 'know', 'what', 'Tom', 'is', 'doing', 'right', 'now']\n",
      "\n",
      " 英文编码后的结果: \n",
      "[1057, 5, 734, 1013, 673, 302, 484, 66, 993, 29, 199, 1017, 142, 112]\n"
     ]
    }
   ],
   "source": [
    "### 定义文本编码方法\n",
    "def text_encode(lang2idx, language):\n",
    "    text_digit = []\n",
    "    for txt in language:\n",
    "        t_digit = []\n",
    "        for t in txt:\n",
    "            t_digit.append(lang2idx[t])\n",
    "        text_digit.append(t_digit)\n",
    "    return text_digit\n",
    "\n",
    "chs_digit = text_encode(chs2idx, Chinese)\n",
    "eng_digit = text_encode(eng2idx, English)\n",
    "print(\"原始中文: \")\n",
    "print(Chinese[20000])\n",
    "print(\"\\n 中文编码后的结果: \")\n",
    "print(chs_digit[20000][:])\n",
    "print(\"原始英文: \")\n",
    "print(English[20000])\n",
    "print(\"\\n 英文编码后的结果: \")\n",
    "print(eng_digit[20000][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始中文: \n",
      "['B', '如果', '我', '是', '你', '，', '我', '不会', '想', '去', '知道', 'Tom', '现在', '正在', '做', '什么', '。']\n",
      "变量X_chs: \n",
      "[1, 917, 12, 35, 5, 79, 12, 16, 257, 36, 73, 202, 429, 496, 129, 299, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "变量X_eng: \n",
      "[1057, 5, 734, 1013, 673, 302, 484, 66, 993, 29, 199, 1017, 142, 112, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "变量Y: \n",
      "3\n"
     ]
    }
   ],
   "source": [
    "### 定义生成训练输入输出序列函数\n",
    "def generate_XY(chs_digit, eng_digit, max_len):\n",
    "    X = []\n",
    "    Y = []\n",
    "    i = -1\n",
    "    for c_digit in chs_digit:\n",
    "        i += 1\n",
    "        Y.append(c_digit[-1])\n",
    "        # 将最后一个字符之前的部分作为X，并补齐字符\n",
    "        x1 = c_digit[:-1] + [0]*(max_len - len(c_digit))\n",
    "        x2 = eng_digit[i] + [0]*(max_len - len(eng_digit[i]))\n",
    "        X.append(x1 + x2)    \n",
    "    return X, Y\n",
    "\n",
    "X, Y = generate_XY(chs_digit, eng_digit, max_len=40)\n",
    "print(\"原始中文: \")\n",
    "print(Chinese[20000])\n",
    "print(\"变量X_chs: \")\n",
    "print(X[20000][0:39])\n",
    "print(\"变量X_eng: \")\n",
    "print(X[20000][-40:])\n",
    "print(\"变量Y: \")\n",
    "print(Y[20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和验证集\n",
    "# 将所有数据的顺序打乱重排\n",
    "idx = np.random.permutation(range(len(X)))\n",
    "X = [X[i] for i in idx]\n",
    "Y = [Y[i] for i in idx]\n",
    "\n",
    "# 切分出1/5的数据作为验证集  \n",
    "validX = X[: len(X) // 5]\n",
    "trainX = X[len(X) // 5 :]\n",
    "validY = Y[: len(Y) // 5]\n",
    "trainY = Y[len(Y) // 5 :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定batch size\n",
    "batch_size = 64\n",
    "# 创建Tensor形式的训练集\n",
    "train_ds = DataSet.TensorDataset(torch.LongTensor(np.array(trainX, dtype=int)), \n",
    "                                 torch.LongTensor(np.array(trainY, dtype=int)))\n",
    "# 形成训练数据加载器\n",
    "train_loader = DataSet.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "# 创建Tensor形式的验证集\n",
    "valid_ds = DataSet.TensorDataset(torch.LongTensor(np.array(validX, dtype=int)), \n",
    "                                 torch.LongTensor(np.array(validY, dtype=int)))\n",
    "# 形成验证数据加载器\n",
    "valid_loader = DataSet.DataLoader(valid_ds, batch_size=batch_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义机器翻译网络结构\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self, eng_vocab_size, chs_vocab_size, embedding_size, hidden_size, num_layers=1):\n",
    "        super(Translator, self).__init__()        \n",
    "        # Pytorch的LSTM层，batch_first标识可以让输入的张量的第一个维度表示batch指标\n",
    "        self.encoder_embedding = nn.Embedding(eng_vocab_size, embedding_size)\n",
    "        self.encoder_lstm = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.decoder_embedding = nn.Embedding(chs_vocab_size, embedding_size)\n",
    "        self.decoder_lstm = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, chs_vocab_size)\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.eng_vocab_size = eng_vocab_size\n",
    "        self.chs_vocab_size = chs_vocab_size\n",
    "    \n",
    "    # 定义前向计算流程\n",
    "    def forward(self, chs, encoder_state):        \n",
    "        # 输入为代表音乐家的one-hot向量，转为具有一定意义的特征向量\n",
    "        x = self.decoder_embedding(chs)\n",
    "        # 读入隐含层的初始信息\n",
    "        hh = encoder_state\n",
    "        # 从输入到隐含层的计算\n",
    "        # x的尺寸为：batch_size，num_step，hidden_size\n",
    "        output, hidden = self.decoder_lstm(x, hh)\n",
    "        # 从output中去除最后一个时间步的数值（output中包含了所有时间步的结果）\n",
    "        output = output[:, -1, ...]\n",
    "        # output的尺寸为：batch_size，hidden_size\n",
    "        # 最后一层全连接网络\n",
    "        output = self.fc(output)\n",
    "        # output的尺寸为：batch_size，output_size\n",
    "        return output\n",
    "    \n",
    "    # 定义隐含单元初始化方法\n",
    "    def initHidden(self, eng, batch_size):       \n",
    "        x = self.encoder_embedding(eng).cuda()    \n",
    "        # 初始化的隐藏元和记忆元,通常它们的维度是一样的\n",
    "        h1 = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size)).cuda()\n",
    "        c1 = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size)).cuda()\n",
    "        #这里我们要对后面的LSTM模型的隐藏状态进行条件初始化\n",
    "        _, encoder_state = self.encoder_lstm(x, (h1, c1))   \n",
    "        return encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translator(\n",
      "  (encoder_embedding): Embedding(7814, 64)\n",
      "  (encoder_lstm): LSTM(64, 128, batch_first=True)\n",
      "  (decoder_embedding): Embedding(13683, 64)\n",
      "  (decoder_lstm): LSTM(64, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=13683, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#给定超参数\n",
    "lr = 1e-2\n",
    "epochs = 200\n",
    "# 创建机器翻译模型实例\n",
    "translator = Translator(eng_vocab_size=eng_vocab_size, chs_vocab_size=chs_vocab_size, embedding_size=64, hidden_size=128)\n",
    "# 转为GPU下的模型\n",
    "translator = translator.cuda()\n",
    "# 交叉熵损失函数\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "# sgd优化算法\n",
    "optimizer = torch.optim.SGD(translator.parameters(), lr=lr) \n",
    "#查看模型具体信息\n",
    "print(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义预测准确率的函数\n",
    "def accuracy(pre, label):\n",
    "    # 得到每一行（每一个样本）输出值最大元素的下标\n",
    "    pre = torch.max(pre.data, 1)[1]\n",
    "    # 将下标与label比较，计算正确的数量\n",
    "    rights = pre.eq(label.data).sum()\n",
    "    # 计算正确预测所占百分比\n",
    "    acc = rights.data / len(label)\n",
    "    return acc.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义一个tensor分割函数\n",
    "def split_chs_eng(x):\n",
    "    x = x.tolist()\n",
    "    x1 = [x[i][0:40] for i in range(len(x))]\n",
    "    x2 = [x[i][-40:] for i in range(len(x))]\n",
    "    x1 = torch.LongTensor(np.array(x1, dtype=int))\n",
    "    x2 = torch.LongTensor(np.array(x2, dtype=int))\n",
    "    return Variable(x1).cuda(), Variable(x2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义训练过程打印函数\n",
    "def print_log(epoch, train_time, train_loss, train_acc, val_loss, val_acc, epochs=10):\n",
    "    print(f\"Epoch [{epoch}/{epochs}], time: {train_time:.2f}s, loss: {train_loss:.4f}, acc: {train_acc:.4f}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义模型验证过程\n",
    "def validate(model, val_loader):\n",
    "    # 在验证集上运行一遍并计算损失和准确率\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.eval()\n",
    "    for batch, data in enumerate(val_loader):\n",
    "        x, y = Variable(data[0]), Variable(data[1])\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        chs, eng = split_chs_eng(x)\n",
    "        encoder_state = model.initHidden(eng, len(data[0]))\n",
    "        outputs = model(chs, encoder_state)\n",
    "        y = y.long()\n",
    "        loss = criterion(outputs, y)\n",
    "        val_loss += loss.data.cpu().numpy()  \n",
    "        val_acc += accuracy(outputs, y)\n",
    "    # 计算平均损失\n",
    "    val_loss /= len(val_loader)  \n",
    "    # 计算平均准确率\n",
    "    val_acc /= len(val_loader)  \n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义模型训练函数\n",
    "def train(model, optimizer, train_loader, val_loader, epochs=50):  \n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        # 记录当前epoch开始时间\n",
    "        start = time.time()  \n",
    "        for batch, data in enumerate(train_loader):\n",
    "            # batch为数字，表示已经进行了几个batch\n",
    "            # data为一个二元组，存储了一个样本的输入和标签\n",
    "            model.train() \n",
    "            x, y = Variable(data[0]), Variable(data[1])\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            chs, eng = split_chs_eng(x)\n",
    "            encoder_state = model.initHidden(eng, len(data[0]))\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(chs, encoder_state)\n",
    "            # 计算当前损失\n",
    "            loss = criterion(outputs, y) \n",
    "            train_loss += loss.data.cpu().numpy()  \n",
    "            train_acc += accuracy(outputs, y) \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            \n",
    "        # 记录当前epoch结束时间\n",
    "        end = time.time()  \n",
    "        # 计算当前epoch的训练耗时 \n",
    "        train_time = end - start  \n",
    "        # 计算平均损失\n",
    "        train_loss /= len(train_loader) \n",
    "        # 计算平均准确率 \n",
    "        train_acc /= len(train_loader) \n",
    "        # 计算验证集上的损失函数和准确率\n",
    "        val_loss, val_acc = validate(model, val_loader)  \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc) \n",
    "        print_log(epoch + 1, train_time, train_loss, train_acc, val_loss, val_acc, epochs=epochs)  \n",
    "        \n",
    "    return train_losses, train_accs, val_losses, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], time: 2.33s, loss: 3.1580, acc: 0.8183, val_loss: 0.9819, val_acc: 0.8360\n",
      "Epoch [2/200], time: 2.33s, loss: 0.7796, acc: 0.8475, val_loss: 0.7674, val_acc: 0.8354\n",
      "Epoch [3/200], time: 2.33s, loss: 0.6942, acc: 0.8475, val_loss: 0.6944, val_acc: 0.8360\n",
      "Epoch [4/200], time: 2.35s, loss: 0.6284, acc: 0.8462, val_loss: 0.6246, val_acc: 0.8358\n",
      "Epoch [5/200], time: 2.32s, loss: 0.5776, acc: 0.8462, val_loss: 0.5799, val_acc: 0.8359\n",
      "Epoch [6/200], time: 2.34s, loss: 0.5397, acc: 0.8462, val_loss: 0.5444, val_acc: 0.8359\n",
      "Epoch [7/200], time: 2.36s, loss: 0.5094, acc: 0.8608, val_loss: 0.5202, val_acc: 0.8820\n",
      "Epoch [8/200], time: 2.34s, loss: 0.4781, acc: 0.8690, val_loss: 0.4856, val_acc: 0.8624\n",
      "Epoch [9/200], time: 2.33s, loss: 0.4518, acc: 0.8854, val_loss: 0.4569, val_acc: 0.8894\n",
      "Epoch [10/200], time: 2.35s, loss: 0.4267, acc: 0.8914, val_loss: 0.4368, val_acc: 0.8891\n",
      "Epoch [11/200], time: 2.38s, loss: 0.4090, acc: 0.8971, val_loss: 0.4157, val_acc: 0.9002\n",
      "Epoch [12/200], time: 2.34s, loss: 0.3904, acc: 0.9080, val_loss: 0.4211, val_acc: 0.8900\n",
      "Epoch [13/200], time: 2.41s, loss: 0.3758, acc: 0.9172, val_loss: 0.3877, val_acc: 0.9160\n",
      "Epoch [14/200], time: 2.39s, loss: 0.3656, acc: 0.9210, val_loss: 0.3787, val_acc: 0.9234\n",
      "Epoch [15/200], time: 2.33s, loss: 0.3571, acc: 0.9265, val_loss: 0.3782, val_acc: 0.9189\n",
      "Epoch [16/200], time: 2.35s, loss: 0.3496, acc: 0.9294, val_loss: 0.3635, val_acc: 0.9300\n",
      "Epoch [17/200], time: 2.31s, loss: 0.3432, acc: 0.9307, val_loss: 0.3580, val_acc: 0.9276\n",
      "Epoch [18/200], time: 2.31s, loss: 0.3379, acc: 0.9307, val_loss: 0.3585, val_acc: 0.9304\n",
      "Epoch [19/200], time: 2.33s, loss: 0.3331, acc: 0.9323, val_loss: 0.3493, val_acc: 0.9296\n",
      "Epoch [20/200], time: 2.37s, loss: 0.3292, acc: 0.9333, val_loss: 0.3434, val_acc: 0.9354\n",
      "Epoch [21/200], time: 2.32s, loss: 0.3254, acc: 0.9355, val_loss: 0.3406, val_acc: 0.9353\n",
      "Epoch [22/200], time: 2.31s, loss: 0.3218, acc: 0.9380, val_loss: 0.3371, val_acc: 0.9367\n",
      "Epoch [23/200], time: 2.35s, loss: 0.3185, acc: 0.9384, val_loss: 0.3355, val_acc: 0.9365\n",
      "Epoch [24/200], time: 2.30s, loss: 0.3155, acc: 0.9385, val_loss: 0.3320, val_acc: 0.9368\n",
      "Epoch [25/200], time: 2.39s, loss: 0.3153, acc: 0.9374, val_loss: 0.3288, val_acc: 0.9360\n",
      "Epoch [26/200], time: 2.35s, loss: 0.3105, acc: 0.9385, val_loss: 0.3298, val_acc: 0.9366\n",
      "Epoch [27/200], time: 2.30s, loss: 0.3110, acc: 0.9377, val_loss: 0.3273, val_acc: 0.9364\n",
      "Epoch [28/200], time: 2.28s, loss: 0.3060, acc: 0.9397, val_loss: 0.3237, val_acc: 0.9369\n",
      "Epoch [29/200], time: 2.33s, loss: 0.3049, acc: 0.9400, val_loss: 0.3229, val_acc: 0.9368\n",
      "Epoch [30/200], time: 2.33s, loss: 0.3019, acc: 0.9402, val_loss: 0.3177, val_acc: 0.9378\n",
      "Epoch [31/200], time: 2.35s, loss: 0.2997, acc: 0.9404, val_loss: 0.3171, val_acc: 0.9387\n",
      "Epoch [32/200], time: 2.31s, loss: 0.2977, acc: 0.9405, val_loss: 0.3160, val_acc: 0.9387\n",
      "Epoch [33/200], time: 2.35s, loss: 0.2984, acc: 0.9393, val_loss: 0.3138, val_acc: 0.9388\n",
      "Epoch [34/200], time: 2.32s, loss: 0.2945, acc: 0.9404, val_loss: 0.3106, val_acc: 0.9385\n",
      "Epoch [35/200], time: 2.32s, loss: 0.2933, acc: 0.9408, val_loss: 0.3095, val_acc: 0.9382\n",
      "Epoch [36/200], time: 2.29s, loss: 0.2974, acc: 0.9396, val_loss: 0.3124, val_acc: 0.9375\n",
      "Epoch [37/200], time: 2.31s, loss: 0.2905, acc: 0.9410, val_loss: 0.3100, val_acc: 0.9396\n",
      "Epoch [38/200], time: 2.31s, loss: 0.2882, acc: 0.9412, val_loss: 0.3054, val_acc: 0.9388\n",
      "Epoch [39/200], time: 2.35s, loss: 0.2872, acc: 0.9411, val_loss: 0.3046, val_acc: 0.9387\n",
      "Epoch [40/200], time: 2.33s, loss: 0.2861, acc: 0.9413, val_loss: 0.3034, val_acc: 0.9393\n",
      "Epoch [41/200], time: 2.39s, loss: 0.2847, acc: 0.9419, val_loss: 0.3046, val_acc: 0.9398\n",
      "Epoch [42/200], time: 2.45s, loss: 0.2840, acc: 0.9415, val_loss: 0.2999, val_acc: 0.9393\n",
      "Epoch [43/200], time: 2.96s, loss: 0.2824, acc: 0.9418, val_loss: 0.2997, val_acc: 0.9400\n",
      "Epoch [44/200], time: 3.04s, loss: 0.2817, acc: 0.9421, val_loss: 0.3012, val_acc: 0.9401\n",
      "Epoch [45/200], time: 3.02s, loss: 0.2806, acc: 0.9423, val_loss: 0.2976, val_acc: 0.9407\n",
      "Epoch [46/200], time: 3.08s, loss: 0.2798, acc: 0.9424, val_loss: 0.2969, val_acc: 0.9406\n",
      "Epoch [47/200], time: 3.01s, loss: 0.2786, acc: 0.9426, val_loss: 0.2956, val_acc: 0.9409\n",
      "Epoch [48/200], time: 3.03s, loss: 0.2776, acc: 0.9423, val_loss: 0.2973, val_acc: 0.9407\n",
      "Epoch [49/200], time: 2.94s, loss: 0.2768, acc: 0.9426, val_loss: 0.2933, val_acc: 0.9409\n",
      "Epoch [50/200], time: 3.01s, loss: 0.2880, acc: 0.9413, val_loss: 0.2948, val_acc: 0.9401\n",
      "Epoch [51/200], time: 3.09s, loss: 0.2751, acc: 0.9427, val_loss: 0.2929, val_acc: 0.9409\n",
      "Epoch [52/200], time: 3.08s, loss: 0.2934, acc: 0.9415, val_loss: 0.2943, val_acc: 0.9397\n",
      "Epoch [53/200], time: 3.05s, loss: 0.2795, acc: 0.9414, val_loss: 0.2916, val_acc: 0.9409\n",
      "Epoch [54/200], time: 3.03s, loss: 0.2787, acc: 0.9415, val_loss: 0.2937, val_acc: 0.9410\n",
      "Epoch [55/200], time: 3.09s, loss: 0.2720, acc: 0.9429, val_loss: 0.2909, val_acc: 0.9408\n",
      "Epoch [56/200], time: 3.04s, loss: 0.2784, acc: 0.9416, val_loss: 0.2894, val_acc: 0.9411\n",
      "Epoch [57/200], time: 2.97s, loss: 0.2706, acc: 0.9430, val_loss: 0.2879, val_acc: 0.9411\n",
      "Epoch [58/200], time: 3.00s, loss: 0.2699, acc: 0.9429, val_loss: 0.2877, val_acc: 0.9412\n",
      "Epoch [59/200], time: 3.06s, loss: 0.2692, acc: 0.9431, val_loss: 0.2876, val_acc: 0.9413\n",
      "Epoch [60/200], time: 3.03s, loss: 0.2686, acc: 0.9433, val_loss: 0.2877, val_acc: 0.9413\n",
      "Epoch [61/200], time: 3.06s, loss: 0.2683, acc: 0.9432, val_loss: 0.2849, val_acc: 0.9413\n",
      "Epoch [62/200], time: 3.01s, loss: 0.2671, acc: 0.9435, val_loss: 0.2848, val_acc: 0.9420\n",
      "Epoch [63/200], time: 3.04s, loss: 0.2673, acc: 0.9435, val_loss: 0.3007, val_acc: 0.9404\n",
      "Epoch [64/200], time: 3.04s, loss: 0.2690, acc: 0.9421, val_loss: 0.2881, val_acc: 0.9418\n",
      "Epoch [65/200], time: 3.06s, loss: 0.2658, acc: 0.9437, val_loss: 0.2840, val_acc: 0.9416\n",
      "Epoch [66/200], time: 3.02s, loss: 0.2654, acc: 0.9437, val_loss: 0.2876, val_acc: 0.9412\n",
      "Epoch [67/200], time: 3.06s, loss: 0.2730, acc: 0.9425, val_loss: 0.2864, val_acc: 0.9408\n",
      "Epoch [68/200], time: 3.08s, loss: 0.2643, acc: 0.9439, val_loss: 0.2826, val_acc: 0.9421\n",
      "Epoch [69/200], time: 3.04s, loss: 0.2634, acc: 0.9438, val_loss: 0.2818, val_acc: 0.9421\n",
      "Epoch [70/200], time: 3.02s, loss: 0.2631, acc: 0.9440, val_loss: 0.2814, val_acc: 0.9421\n",
      "Epoch [71/200], time: 3.00s, loss: 0.2623, acc: 0.9442, val_loss: 0.2832, val_acc: 0.9421\n",
      "Epoch [72/200], time: 2.96s, loss: 0.2619, acc: 0.9439, val_loss: 0.2818, val_acc: 0.9423\n",
      "Epoch [73/200], time: 2.92s, loss: 0.2615, acc: 0.9443, val_loss: 0.2799, val_acc: 0.9420\n",
      "Epoch [74/200], time: 3.00s, loss: 0.2611, acc: 0.9442, val_loss: 0.2799, val_acc: 0.9418\n",
      "Epoch [75/200], time: 3.04s, loss: 0.2614, acc: 0.9444, val_loss: 0.2816, val_acc: 0.9422\n",
      "Epoch [76/200], time: 3.00s, loss: 0.2602, acc: 0.9445, val_loss: 0.2796, val_acc: 0.9421\n",
      "Epoch [77/200], time: 3.00s, loss: 0.2599, acc: 0.9443, val_loss: 0.2778, val_acc: 0.9419\n",
      "Epoch [78/200], time: 3.06s, loss: 0.2593, acc: 0.9445, val_loss: 0.2792, val_acc: 0.9421\n",
      "Epoch [79/200], time: 3.00s, loss: 0.2588, acc: 0.9444, val_loss: 0.2792, val_acc: 0.9422\n",
      "Epoch [80/200], time: 3.04s, loss: 0.2587, acc: 0.9447, val_loss: 0.2779, val_acc: 0.9423\n",
      "Epoch [81/200], time: 2.95s, loss: 0.2583, acc: 0.9445, val_loss: 0.2786, val_acc: 0.9424\n",
      "Epoch [82/200], time: 2.94s, loss: 0.2578, acc: 0.9445, val_loss: 0.2765, val_acc: 0.9436\n",
      "Epoch [83/200], time: 2.96s, loss: 0.2574, acc: 0.9444, val_loss: 0.2768, val_acc: 0.9439\n",
      "Epoch [84/200], time: 2.95s, loss: 0.2570, acc: 0.9448, val_loss: 0.2790, val_acc: 0.9421\n",
      "Epoch [85/200], time: 3.06s, loss: 0.2583, acc: 0.9436, val_loss: 0.2911, val_acc: 0.9407\n",
      "Epoch [86/200], time: 3.01s, loss: 0.2627, acc: 0.9433, val_loss: 0.2804, val_acc: 0.9429\n",
      "Epoch [87/200], time: 2.98s, loss: 0.2571, acc: 0.9451, val_loss: 0.2751, val_acc: 0.9430\n",
      "Epoch [88/200], time: 3.01s, loss: 0.2562, acc: 0.9447, val_loss: 0.2744, val_acc: 0.9440\n",
      "Epoch [89/200], time: 2.99s, loss: 0.2552, acc: 0.9452, val_loss: 0.2776, val_acc: 0.9408\n",
      "Epoch [90/200], time: 2.41s, loss: 0.2551, acc: 0.9449, val_loss: 0.2757, val_acc: 0.9433\n",
      "Epoch [91/200], time: 2.32s, loss: 0.2553, acc: 0.9451, val_loss: 0.2754, val_acc: 0.9422\n",
      "Epoch [92/200], time: 2.31s, loss: 0.2544, acc: 0.9455, val_loss: 0.2735, val_acc: 0.9448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [93/200], time: 2.34s, loss: 0.2584, acc: 0.9441, val_loss: 0.2780, val_acc: 0.9432\n",
      "Epoch [94/200], time: 2.36s, loss: 0.2546, acc: 0.9459, val_loss: 0.2738, val_acc: 0.9440\n",
      "Epoch [95/200], time: 2.33s, loss: 0.2539, acc: 0.9460, val_loss: 0.2747, val_acc: 0.9430\n",
      "Epoch [96/200], time: 2.34s, loss: 0.2706, acc: 0.9449, val_loss: 0.2754, val_acc: 0.9438\n",
      "Epoch [97/200], time: 2.36s, loss: 0.2527, acc: 0.9459, val_loss: 0.2738, val_acc: 0.9444\n",
      "Epoch [98/200], time: 2.35s, loss: 0.2526, acc: 0.9460, val_loss: 0.2727, val_acc: 0.9447\n",
      "Epoch [99/200], time: 2.31s, loss: 0.2518, acc: 0.9465, val_loss: 0.2732, val_acc: 0.9434\n",
      "Epoch [100/200], time: 2.32s, loss: 0.2515, acc: 0.9464, val_loss: 0.2724, val_acc: 0.9448\n",
      "Epoch [101/200], time: 2.34s, loss: 0.2515, acc: 0.9464, val_loss: 0.2719, val_acc: 0.9449\n",
      "Epoch [102/200], time: 2.36s, loss: 0.2510, acc: 0.9464, val_loss: 0.2732, val_acc: 0.9443\n",
      "Epoch [103/200], time: 2.42s, loss: 0.2508, acc: 0.9465, val_loss: 0.2730, val_acc: 0.9445\n",
      "Epoch [104/200], time: 2.33s, loss: 0.2504, acc: 0.9465, val_loss: 0.2726, val_acc: 0.9435\n",
      "Epoch [105/200], time: 2.29s, loss: 0.2505, acc: 0.9467, val_loss: 0.2715, val_acc: 0.9447\n",
      "Epoch [106/200], time: 2.29s, loss: 0.2515, acc: 0.9454, val_loss: 0.2765, val_acc: 0.9424\n",
      "Epoch [107/200], time: 2.38s, loss: 0.2496, acc: 0.9466, val_loss: 0.2710, val_acc: 0.9446\n",
      "Epoch [108/200], time: 2.33s, loss: 0.2493, acc: 0.9466, val_loss: 0.2703, val_acc: 0.9447\n",
      "Epoch [109/200], time: 2.35s, loss: 0.2493, acc: 0.9466, val_loss: 0.2710, val_acc: 0.9446\n",
      "Epoch [110/200], time: 2.33s, loss: 0.2488, acc: 0.9467, val_loss: 0.2708, val_acc: 0.9445\n",
      "Epoch [111/200], time: 2.34s, loss: 0.2486, acc: 0.9468, val_loss: 0.2695, val_acc: 0.9446\n",
      "Epoch [112/200], time: 2.34s, loss: 0.2481, acc: 0.9467, val_loss: 0.2705, val_acc: 0.9447\n",
      "Epoch [113/200], time: 2.35s, loss: 0.2477, acc: 0.9465, val_loss: 0.2698, val_acc: 0.9443\n",
      "Epoch [114/200], time: 2.36s, loss: 0.2476, acc: 0.9466, val_loss: 0.2701, val_acc: 0.9441\n",
      "Epoch [115/200], time: 2.41s, loss: 0.2475, acc: 0.9467, val_loss: 0.2685, val_acc: 0.9450\n",
      "Epoch [116/200], time: 2.32s, loss: 0.2477, acc: 0.9468, val_loss: 0.2691, val_acc: 0.9445\n",
      "Epoch [117/200], time: 2.33s, loss: 0.2470, acc: 0.9468, val_loss: 0.2683, val_acc: 0.9447\n",
      "Epoch [118/200], time: 2.32s, loss: 0.2466, acc: 0.9469, val_loss: 0.2700, val_acc: 0.9444\n",
      "Epoch [119/200], time: 2.32s, loss: 0.2463, acc: 0.9469, val_loss: 0.2680, val_acc: 0.9447\n",
      "Epoch [120/200], time: 2.36s, loss: 0.2461, acc: 0.9469, val_loss: 0.2699, val_acc: 0.9447\n",
      "Epoch [121/200], time: 2.33s, loss: 0.2459, acc: 0.9470, val_loss: 0.2703, val_acc: 0.9446\n",
      "Epoch [122/200], time: 2.40s, loss: 0.2456, acc: 0.9469, val_loss: 0.2698, val_acc: 0.9446\n",
      "Epoch [123/200], time: 2.39s, loss: 0.2453, acc: 0.9471, val_loss: 0.2692, val_acc: 0.9447\n",
      "Epoch [124/200], time: 2.39s, loss: 0.2453, acc: 0.9470, val_loss: 0.2689, val_acc: 0.9448\n",
      "Epoch [125/200], time: 2.36s, loss: 0.2451, acc: 0.9471, val_loss: 0.2690, val_acc: 0.9445\n",
      "Epoch [126/200], time: 2.36s, loss: 0.2472, acc: 0.9458, val_loss: 0.2689, val_acc: 0.9445\n",
      "Epoch [127/200], time: 2.36s, loss: 0.2449, acc: 0.9471, val_loss: 0.2670, val_acc: 0.9446\n",
      "Epoch [128/200], time: 2.36s, loss: 0.2445, acc: 0.9473, val_loss: 0.2665, val_acc: 0.9447\n",
      "Epoch [129/200], time: 2.40s, loss: 0.2437, acc: 0.9471, val_loss: 0.2729, val_acc: 0.9434\n",
      "Epoch [130/200], time: 2.37s, loss: 0.2476, acc: 0.9460, val_loss: 0.2690, val_acc: 0.9443\n",
      "Epoch [131/200], time: 2.36s, loss: 0.2436, acc: 0.9473, val_loss: 0.2694, val_acc: 0.9441\n",
      "Epoch [132/200], time: 2.35s, loss: 0.2434, acc: 0.9473, val_loss: 0.2665, val_acc: 0.9443\n",
      "Epoch [133/200], time: 2.33s, loss: 0.2429, acc: 0.9471, val_loss: 0.2661, val_acc: 0.9443\n",
      "Epoch [134/200], time: 2.36s, loss: 0.2482, acc: 0.9460, val_loss: 0.2726, val_acc: 0.9436\n",
      "Epoch [135/200], time: 2.35s, loss: 0.2469, acc: 0.9461, val_loss: 0.3141, val_acc: 0.9106\n",
      "Epoch [136/200], time: 2.31s, loss: 0.2431, acc: 0.9472, val_loss: 0.2682, val_acc: 0.9448\n",
      "Epoch [137/200], time: 2.34s, loss: 0.2424, acc: 0.9473, val_loss: 0.2666, val_acc: 0.9447\n",
      "Epoch [138/200], time: 2.30s, loss: 0.2418, acc: 0.9473, val_loss: 0.2657, val_acc: 0.9448\n",
      "Epoch [139/200], time: 2.38s, loss: 0.2416, acc: 0.9472, val_loss: 0.2648, val_acc: 0.9451\n",
      "Epoch [140/200], time: 2.32s, loss: 0.2414, acc: 0.9473, val_loss: 0.2652, val_acc: 0.9451\n",
      "Epoch [141/200], time: 2.36s, loss: 0.2412, acc: 0.9473, val_loss: 0.2653, val_acc: 0.9448\n",
      "Epoch [142/200], time: 2.31s, loss: 0.2411, acc: 0.9474, val_loss: 0.2643, val_acc: 0.9448\n",
      "Epoch [143/200], time: 2.36s, loss: 0.2411, acc: 0.9473, val_loss: 0.2654, val_acc: 0.9448\n",
      "Epoch [144/200], time: 2.34s, loss: 0.2406, acc: 0.9474, val_loss: 0.2648, val_acc: 0.9448\n",
      "Epoch [145/200], time: 2.33s, loss: 0.2405, acc: 0.9474, val_loss: 0.2671, val_acc: 0.9449\n",
      "Epoch [146/200], time: 2.33s, loss: 0.2405, acc: 0.9475, val_loss: 0.2666, val_acc: 0.9434\n",
      "Epoch [147/200], time: 2.34s, loss: 0.2403, acc: 0.9475, val_loss: 0.2650, val_acc: 0.9447\n",
      "Epoch [148/200], time: 2.41s, loss: 0.2399, acc: 0.9474, val_loss: 0.2645, val_acc: 0.9451\n",
      "Epoch [149/200], time: 2.34s, loss: 0.2395, acc: 0.9477, val_loss: 0.2645, val_acc: 0.9445\n",
      "Epoch [150/200], time: 2.31s, loss: 0.2396, acc: 0.9478, val_loss: 0.2653, val_acc: 0.9449\n",
      "Epoch [151/200], time: 2.39s, loss: 0.2394, acc: 0.9476, val_loss: 0.2640, val_acc: 0.9451\n",
      "Epoch [152/200], time: 2.38s, loss: 0.2390, acc: 0.9477, val_loss: 0.2627, val_acc: 0.9453\n",
      "Epoch [153/200], time: 2.35s, loss: 0.2391, acc: 0.9476, val_loss: 0.2660, val_acc: 0.9447\n",
      "Epoch [154/200], time: 2.35s, loss: 0.2434, acc: 0.9463, val_loss: 0.2866, val_acc: 0.9440\n",
      "Epoch [155/200], time: 2.33s, loss: 0.2422, acc: 0.9465, val_loss: 0.2684, val_acc: 0.9436\n",
      "Epoch [156/200], time: 2.33s, loss: 0.2385, acc: 0.9478, val_loss: 0.2623, val_acc: 0.9452\n",
      "Epoch [157/200], time: 2.33s, loss: 0.2378, acc: 0.9479, val_loss: 0.2637, val_acc: 0.9450\n",
      "Epoch [158/200], time: 2.37s, loss: 0.2379, acc: 0.9478, val_loss: 0.2637, val_acc: 0.9451\n",
      "Epoch [159/200], time: 2.31s, loss: 0.2377, acc: 0.9478, val_loss: 0.2654, val_acc: 0.9451\n",
      "Epoch [160/200], time: 2.31s, loss: 0.2377, acc: 0.9482, val_loss: 0.2635, val_acc: 0.9453\n",
      "Epoch [161/200], time: 2.31s, loss: 0.2375, acc: 0.9478, val_loss: 0.2631, val_acc: 0.9448\n",
      "Epoch [162/200], time: 2.40s, loss: 0.2429, acc: 0.9466, val_loss: 0.2650, val_acc: 0.9454\n",
      "Epoch [163/200], time: 2.37s, loss: 0.2417, acc: 0.9469, val_loss: 0.2760, val_acc: 0.9447\n",
      "Epoch [164/200], time: 2.42s, loss: 0.2373, acc: 0.9482, val_loss: 0.2618, val_acc: 0.9461\n",
      "Epoch [165/200], time: 2.37s, loss: 0.2369, acc: 0.9483, val_loss: 0.2654, val_acc: 0.9452\n",
      "Epoch [166/200], time: 2.40s, loss: 0.2366, acc: 0.9482, val_loss: 0.2629, val_acc: 0.9451\n",
      "Epoch [167/200], time: 2.38s, loss: 0.2366, acc: 0.9481, val_loss: 0.2633, val_acc: 0.9460\n",
      "Epoch [168/200], time: 2.41s, loss: 0.2362, acc: 0.9483, val_loss: 0.2615, val_acc: 0.9461\n",
      "Epoch [169/200], time: 2.35s, loss: 0.2362, acc: 0.9482, val_loss: 0.2612, val_acc: 0.9459\n",
      "Epoch [170/200], time: 2.37s, loss: 0.2358, acc: 0.9482, val_loss: 0.2618, val_acc: 0.9461\n",
      "Epoch [171/200], time: 2.31s, loss: 0.2356, acc: 0.9482, val_loss: 0.2611, val_acc: 0.9459\n",
      "Epoch [172/200], time: 2.40s, loss: 0.2383, acc: 0.9472, val_loss: 0.2638, val_acc: 0.9460\n",
      "Epoch [173/200], time: 2.35s, loss: 0.2354, acc: 0.9484, val_loss: 0.2615, val_acc: 0.9462\n",
      "Epoch [174/200], time: 2.37s, loss: 0.2354, acc: 0.9483, val_loss: 0.2614, val_acc: 0.9460\n",
      "Epoch [175/200], time: 2.39s, loss: 0.2354, acc: 0.9485, val_loss: 0.2606, val_acc: 0.9460\n",
      "Epoch [176/200], time: 2.34s, loss: 0.2347, acc: 0.9485, val_loss: 0.2629, val_acc: 0.9465\n",
      "Epoch [177/200], time: 2.40s, loss: 0.2346, acc: 0.9484, val_loss: 0.2661, val_acc: 0.9441\n",
      "Epoch [178/200], time: 2.33s, loss: 0.2345, acc: 0.9482, val_loss: 0.2618, val_acc: 0.9458\n",
      "Epoch [179/200], time: 2.32s, loss: 0.2366, acc: 0.9472, val_loss: 0.2630, val_acc: 0.9461\n",
      "Epoch [180/200], time: 2.36s, loss: 0.2345, acc: 0.9484, val_loss: 0.2602, val_acc: 0.9459\n",
      "Epoch [181/200], time: 2.31s, loss: 0.2339, acc: 0.9485, val_loss: 0.2610, val_acc: 0.9463\n",
      "Epoch [182/200], time: 2.33s, loss: 0.2341, acc: 0.9485, val_loss: 0.2609, val_acc: 0.9461\n",
      "Epoch [183/200], time: 2.31s, loss: 0.2337, acc: 0.9484, val_loss: 0.2609, val_acc: 0.9460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [184/200], time: 2.33s, loss: 0.2495, acc: 0.9474, val_loss: 0.2646, val_acc: 0.9456\n",
      "Epoch [185/200], time: 2.45s, loss: 0.2364, acc: 0.9472, val_loss: 0.2627, val_acc: 0.9460\n",
      "Epoch [186/200], time: 2.30s, loss: 0.2335, acc: 0.9486, val_loss: 0.2600, val_acc: 0.9458\n",
      "Epoch [187/200], time: 2.36s, loss: 0.2330, acc: 0.9486, val_loss: 0.2611, val_acc: 0.9463\n",
      "Epoch [188/200], time: 2.36s, loss: 0.2328, acc: 0.9485, val_loss: 0.2608, val_acc: 0.9456\n",
      "Epoch [189/200], time: 2.35s, loss: 0.2326, acc: 0.9485, val_loss: 0.2596, val_acc: 0.9459\n",
      "Epoch [190/200], time: 2.39s, loss: 0.2427, acc: 0.9461, val_loss: 0.2830, val_acc: 0.9448\n",
      "Epoch [191/200], time: 2.35s, loss: 0.2335, acc: 0.9486, val_loss: 0.2598, val_acc: 0.9462\n",
      "Epoch [192/200], time: 2.36s, loss: 0.2324, acc: 0.9484, val_loss: 0.2597, val_acc: 0.9458\n",
      "Epoch [193/200], time: 2.32s, loss: 0.2325, acc: 0.9484, val_loss: 0.2611, val_acc: 0.9456\n",
      "Epoch [194/200], time: 2.36s, loss: 0.2323, acc: 0.9486, val_loss: 0.2600, val_acc: 0.9458\n",
      "Epoch [195/200], time: 2.36s, loss: 0.2316, acc: 0.9486, val_loss: 0.2606, val_acc: 0.9459\n",
      "Epoch [196/200], time: 2.40s, loss: 0.2317, acc: 0.9484, val_loss: 0.2594, val_acc: 0.9458\n",
      "Epoch [197/200], time: 2.32s, loss: 0.2317, acc: 0.9485, val_loss: 0.2597, val_acc: 0.9465\n",
      "Epoch [198/200], time: 2.41s, loss: 0.2316, acc: 0.9486, val_loss: 0.2591, val_acc: 0.9459\n",
      "Epoch [199/200], time: 2.38s, loss: 0.2315, acc: 0.9485, val_loss: 0.2608, val_acc: 0.9463\n",
      "Epoch [200/200], time: 2.34s, loss: 0.2312, acc: 0.9486, val_loss: 0.2599, val_acc: 0.9461\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "history = train(translator, optimizer, train_loader, valid_loader, epochs=epochs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？知識康复\n"
     ]
    }
   ],
   "source": [
    "### 模型翻译测试\n",
    "max_len = 40\n",
    "test = 'what is it'\n",
    "test = test.split(' ')\n",
    "eng = []\n",
    "for t in test:\n",
    "    eng.append(eng2idx[t])\n",
    "    \n",
    "eng = eng + [0]*(max_len - len(eng))\n",
    "eng = torch.LongTensor(np.array([eng], dtype=int))\n",
    "eng = Variable(eng).cuda()\n",
    "predict = [chs2idx['B']]*(max_len - 1)\n",
    "predict = np.array([int(i.cpu()) if type(i)!=int else i for i in predict])\n",
    "chs = ''\n",
    "\n",
    "for i in range(max_len - 2):\n",
    "    encoder_state = translator.initHidden(eng, 1)\n",
    "    pre = torch.LongTensor(np.array([predict], dtype=int))\n",
    "    pre = Variable(pre).cuda()\n",
    "    output = translator(pre, encoder_state)\n",
    "    # 提取最大概率的字符所在的位置，记录其编号\n",
    "    index = torch.argmax(output) \n",
    "    predict[i+1] = index\n",
    "    \n",
    "    if predict[i+1] == 0:\n",
    "        break\n",
    "        \n",
    "    # 提取上述编号所对应的字符\n",
    "    current_word = [k for k, v in chs2idx.items() if v==index][0] \n",
    "    chs = chs + current_word\n",
    "    \n",
    "print(chs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
